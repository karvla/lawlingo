{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The usage of blacklisted words in Swedish judgements\n",
    "The Swedish Justice Department has published a [list](https://www.regeringen.se/rapporter/2011/10/pm-20111/) of words that can be replaced to improve readability. This notebook is used to track the occurences of these words in public judgments. This tool was used in a project by Miriam Öhman at Lund University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep\n",
    "import os\n",
    "import urllib.request\n",
    "from urllib import parse\n",
    "import urllib.parse\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from joblib import Memory\n",
    "from datetime import datetime\n",
    "import textract\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "memory = Memory('./cache', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def text_from_url(url):\n",
    "    urllib.request.urlretrieve(url, \"./cache/temp.pdf\")\n",
    "    return textract.process('./cache/temp.pdf', encoding='utf-16').decode('utf-16')\n",
    "\n",
    "@memory.cache\n",
    "def get_domstol_se(section_url):\n",
    "    BASE_URL = \"https://www.domstol.se/\"\n",
    "    URL = BASE_URL + section_url\n",
    "    BANNER_CLASS_NAME = \"banner__button\"\n",
    "    MORE_CLASS_NAME = \"search-result-item__show-more-btn\"\n",
    "    \n",
    "    urls = []\n",
    "    driver = webdriver.Chrome('./chromedriver')\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    driver.get(URL)\n",
    "    sleep(1.0)\n",
    "    \n",
    "    banner_button = driver.find_element_by_class_name(BANNER_CLASS_NAME)\n",
    "    more_button = driver.find_element_by_class_name(MORE_CLASS_NAME)\n",
    "    \n",
    "    wait.until(EC.element_to_be_clickable((By.CLASS_NAME, BANNER_CLASS_NAME)))\n",
    "    banner_button.click()\n",
    "        \n",
    "    while True:\n",
    "        more_button.click()\n",
    "        try:\n",
    "            wait.until(EC.element_to_be_clickable((By.CLASS_NAME, MORE_CLASS_NAME)))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "    results = driver.find_elements_by_class_name(\"u-pr-huge--large\")\n",
    "    urls =  [item.get_attribute(\"href\") for item in results]\n",
    "    driver.close()\n",
    "    return tuple(urls)\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def data_from_pdf_page(url):\n",
    "    BASE_URL = \"https://www.domstol.se/\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "    pdf_element = soup.find(\"a\", href=re.compile(r\".+\\.pdf\"))\n",
    "    date_element = soup.find(\"span\", class_ = \"publisher__byline\")\n",
    "   \n",
    "    if pdf_element is None:\n",
    "        print('Failed, no pdf', url)\n",
    "        return None\n",
    "    elif date_element is None:\n",
    "        print('Failed, no date', url)\n",
    "        return None\n",
    "    else:\n",
    "        date = datetime.fromisoformat(date_element.text[:-1])\n",
    "        pdf_url = BASE_URL + pdf_element[\"href\"]\n",
    "        file_name = re.findall(r\"(.+\\/)*(.+\\..+)$\", pdf_url)[0][1]\n",
    "        text = text_from_url(pdf_url)\n",
    "        return {\"date\": date, \"url\": pdf_url, \"text\": text}\n",
    "    \n",
    "def words(text):\n",
    "    \"\"\"\n",
    "    Returns a list of words from a text.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\n', \" \", text)\n",
    "    text = re.sub('[/./?,!/:/(/)_]', \"\", text)\n",
    "    text = re.sub('[0-9]', \"\", text)\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "# This does not work for the urls that contains non-ascii characters. \n",
    "@memory.cache\n",
    "def get_migrations():\n",
    "    data = []\n",
    "    \n",
    "    # From csv\n",
    "    with open(\"./migrations_dommar.csv\", newline='',  encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            date = row['Date']\n",
    "            url = row['Url']\n",
    "            if url[-3:] != \"pdf\":\n",
    "                data.append(data_from_pdf_page(url))\n",
    "                continue\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, \"./cache/temp.pdf\")\n",
    "            except:\n",
    "                print(\"Could not fetch \", date, url)\n",
    "                continue\n",
    "            text = textract.process('./cache/temp.pdf', encoding='utf-16').decode('utf-16')\n",
    "            data.append({'date': datetime.fromisoformat(date), 'url': url, 'text': text})\n",
    "            \n",
    "    # From folder\n",
    "    folder = './missing_mig/'\n",
    "    for file_name in os.listdir(folder):\n",
    "        print(file_name)\n",
    "        date = file_name[:10]\n",
    "        url = folder + file_name\n",
    "        text = textract.process(url, encoding='utf-16').decode('utf-16')\n",
    "        data.append({'date': datetime.fromisoformat(date), 'url': url, 'text': text})\n",
    "    \n",
    "    return tuple(data)\n",
    "\n",
    "@memory.cache\n",
    "def get_arbets():\n",
    "    data = []\n",
    "    URL = \"http://www.arbetsdomstolen.se/pages/page.asp?lngID=4&lngLangID=1&Year={}\"\n",
    "    BASE = \"http://www.arbetsdomstolen.se/pages/page.asp\"\n",
    "    for year in range(2003, 2020):\n",
    "        page = requests.get(URL.format(year))\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        content = soup.find(id=\"content\")\n",
    "\n",
    "        for element in content.find_all(\"li\"):\n",
    "            result_url = element.find(\"a\")['href']\n",
    "            pdf_url = _arbets_pdf(BASE + result_url)\n",
    "            title = element.find(\"strong\").contents[0]\n",
    "            date = re.findall(\"\\d{4}-\\d{2}-\\d{2}\", title)[0]\n",
    "            if pdf_url is not None:\n",
    "                try:\n",
    "                    text = text_from_url(pdf_url)\n",
    "                except:\n",
    "                    print(\"Failed\", pdf_url)\n",
    "                    continue\n",
    "                data.append({\"date\": datetime.fromisoformat(date), \"url\": pdf_url, \"text\": text})\n",
    "                print(\"Downloaded\", pdf_url)\n",
    "\n",
    "    return tuple(data)\n",
    "\n",
    "def _arbets_pdf(url):\n",
    "    BASE = \"http://www.arbetsdomstolen.se/\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        if re.match(r\".+\\.pdf\", str(link.get('href'))):\n",
    "            return BASE + link['href'][2:]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def _id(text):\n",
    "    return re.findall('[A-ZÅÄÖ] \\d{1,4}[-\\/]\\d{2}', text)[0]\n",
    "\n",
    "\n",
    "def _id_mig(text):\n",
    "    return re.findall('[A-Z]{0,3}\\d{1,5}-\\d{1,2}', text)[0]\n",
    "\n",
    "def _id_hfd(text):\n",
    "    try:\n",
    "        return re.findall('(?:[Mm]ål nr\\s+|följande\\sdom\\D{0,10})(\\d+[-–]*\\d+[-–]?\\d*)', text)[0]\n",
    "    except:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and sorting data from decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding decistions from Högsta Domstolen\n",
    "df_hd = pd.DataFrame([data_from_pdf_page(url) for url in get_domstol_se(\"hogsta-domstolen/avgoranden/?f=DecisionType_list:decision\") if data_from_pdf_page(url) is not None])\n",
    "df_hd = df_hd.loc[df_hd['text'].apply(lambda x: not x.isspace())]\n",
    "df_hd[\"type\"] = \"HD\"\n",
    "df_hd[\"id\"] = df_hd['text'].apply(_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding decistions from Högsta förvaltningsdomstolen\n",
    "df_hfd = pd.DataFrame([data_from_pdf_page(url) for url in get_domstol_se(\"hogsta-forvaltningsdomstolen/avgoranden/\") if data_from_pdf_page(url) is not None])\n",
    "df_hfd = df_hfd.loc[df_hfd['text'].apply(lambda x: not x.isspace())]\n",
    "df_hfd[\"type\"] = \"HFD\"\n",
    "df_hfd[\"id\"] = df_hfd['text'].apply(_id_hfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding descisions from Migrationsdomstolen\n",
    "df_mig = pd.DataFrame(get_migrations())\n",
    "df_mig = df_mig.loc[df_mig['text'].apply(lambda x: not x.isspace())]\n",
    "df_mig[\"type\"] = \"MIG\"\n",
    "df_mig[\"id\"] = df_mig['text'].apply(_id_mig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding descisions from Arbetsdomstolen\n",
    "df_ad = pd.DataFrame(get_arbets())\n",
    "df_ad = df_ad.loc[df_ad['text'].apply(lambda x: not x.isspace())]\n",
    "df_ad[\"type\"] = \"AD\"\n",
    "df_ad[\"id\"] = df_ad['text'].apply(_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching and counting blacklisted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting words\n",
    "df = pd.DataFrame().append([df_hd, df_hfd, df_mig, df_ad])\n",
    "df['word_count'] = list(map(lambda x: Counter(words(x)), df['text']))\n",
    "df['n_words'] = list(map(lambda x: len(words(x)), df['text']))\n",
    "\n",
    "# Counting occurences of blacklisted words\n",
    "with open('./svartlistadeord.txt') as f:\n",
    "    illegal_words = tuple(f.read().splitlines())\n",
    "    \n",
    "with open('./svartlistadeordstammar.txt') as f:\n",
    "    illegal_word_stems = tuple(f.read().splitlines())\n",
    "    \n",
    "def illegal_count(text):\n",
    "    counter = Counter()\n",
    "    for word in illegal_word_stems:\n",
    "        key = r\"\\s(\" + word + \"\\p{L}*)\"\n",
    "        matches = re.finditer(key, text.lower())\n",
    "        counter.update([m.group(1) for m in matches])\n",
    "        \n",
    "    for word in illegal_words:\n",
    "        key = r\"\\s(\" + word + \")[\\s|\\!|\\?|\\.|\\,]\"\n",
    "        matches = re.finditer(key, text.lower())\n",
    "        counter.update([m.group(1) for m in matches])\n",
    "        \n",
    "    return counter\n",
    "    \n",
    "df['illegal_count'] = list(map(illegal_count, df['text']))\n",
    "df['n_illegal_occurences'] = list(map(lambda x: sum(x.values()), df['illegal_count']))\n",
    "\n",
    "# Calculating the ratio of blacklisted words\n",
    "df['illegal_ratio'] = df['n_illegal_occurences']/df['n_words']\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using news articles as controll\n",
    "234,196 news articles from 2016 is used as control. The dataset can be found here: \n",
    "https://webhose.io/free-datasets/swedish-news-articles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def mean_ratio_news():\n",
    "    blacklisted = 0\n",
    "    total = 0\n",
    "    for filename in os.listdir('./news'):\n",
    "        with open('./news/' + filename, 'r') as f:\n",
    "            text = json.load(f)['text']\n",
    "        blacklisted += sum(illegal_count(text).values())\n",
    "        total += len(words(text))\n",
    "    \n",
    "    return blacklisted / total\n",
    "            \n",
    "mean_news = mean_ratio_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of decisions of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('type')['text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean percent blacklisted words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('type')['illegal_ratio'].mean().apply(lambda x: str(round(x*100, 2))+\"%\"))\n",
    "print()\n",
    "print(\"Control\\t\",  str(round(mean_news*100, 3))+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting percent blacklisted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "import mpld3\n",
    "mpld3.disable_notebook() # Enable for zoom\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "ax = df.groupby(['type'])['illegal_ratio'].plot(style='o', ms=3, figsize=(10,7))\n",
    "ax[0].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Procent svartlistade ord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decisions for in cluster from HFD spring 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[datetime.fromisoformat(\"2014-03-01\"):datetime.fromisoformat(\"2014-06-01\")]\n",
    "df1.loc[(df1['type'] == \"HFD\") &  (0.0118 < df1['illegal_ratio']) & (df1['illegal_ratio'] < 0.013)][['url', 'illegal_ratio']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... Applying rolling average for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 500\n",
    "ax = df.loc[df['type'] == 'HD']['illegal_ratio'].rolling(win_size, min_periods = 50).mean().plot(label=\"Högsta domstolen\", figsize=(10,7))\n",
    "df.loc[df['type'] == 'AD']['illegal_ratio'].rolling(win_size, min_periods = 50).mean().plot(label=\"Arbetsdomstolen\")\n",
    "df.loc[df['type'] == 'HFD']['illegal_ratio'].rolling(win_size, min_periods = 50).mean().plot(label=\"Högsta förvaltningsdomstolen\")\n",
    "df.loc[df['type'] == 'MIG']['illegal_ratio'].rolling(win_size, min_periods = 50).mean().plot(label=\"Migrationsöverdomstolen\")\n",
    "\n",
    "ax.axhline(y=mean_news, color='r', linestyle='dashed', label=\"Nyhetsartiklar från 2016\")\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "ax.legend()\n",
    "ax.set_title(\"Procent svartlistade ord (rullande medelvärde {} domar)\".format(win_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common blacklisted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for count in df['illegal_count'].iloc:\n",
    "    counter += count\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions of each class with highest percentage of blacklisted words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.reset_index()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df2.loc[df2.groupby(['type'])['illegal_ratio'].idxmax()][['date','url', 'type', 'illegal_ratio', 'n_illegal_occurences']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "Blacklisted words were counted manually to make sure the automatic counter was not to agressive or not agressive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df2.loc[[1645, 1588, 2031, 3252]]\n",
    "df_test['manual_count'] = [25, 15, 47, 37]\n",
    "df_test[['url', 'type', 'n_illegal_occurences', 'manual_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AD\n",
    "df_test['illegal_count'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HD\n",
    "df_test['illegal_count'].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HFD\n",
    "df_test['illegal_count'].values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIG\n",
    "df_test['illegal_count'].values[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving lists of decsitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['type'] == \"HD\"]['id'].to_csv('hd.csv',sep=\" \", encoding='utf-8', header=None)\n",
    "df.loc[df['type'] == \"HFD\"]['id'].to_csv('hfd.csv',sep=\" \", encoding='utf-8', header=None)\n",
    "df.loc[df['type'] == \"MIG\"]['id'].to_csv('mig.csv',sep=\" \", encoding='utf-8', header=None)\n",
    "df.loc[df['type'] == \"AD\"]['id'].to_csv('ad.csv',sep=\" \", encoding='utf-8', header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
